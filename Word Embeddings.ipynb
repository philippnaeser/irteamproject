{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Other than for the probabilistic rating models and TFIDF, we look into the raw text documents provided. Hence, we have first to apply some preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5371"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# Gensim requires list of lists of Unicode 8 strings as an input. Since we have a small collection, we are fine with loading everything into memory.\n",
    "import re\n",
    "doc_list= []\n",
    "with open('/Users/d071503/Desktop/IR/irteamproject/nfcorpus/raw/doc_dump.txt', 'r') as rf1:\n",
    "    for line in rf1:\n",
    "        l = re.sub(\"MED-.*\\t\", \"\",line).lower().strip('\\n').split()\n",
    "        doc_list.append(l) \n",
    "len(doc_list) # TODO: Report this in project report. Maybe also other summary stats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations / Phrases\n",
    "\n",
    "For both models we will extract multi-word expressions first and analogously.\n",
    "We use gensim's phrase detection [module](https://radimrehurek.com/gensim/models/phrases.html#id2).\n",
    "\n",
    "We use gensim's default approach and parameter settings to detect collocations which is outlined [here](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality). \n",
    "\n",
    "The formula applied is:\n",
    "\n",
    "(count(word_a followed by word_b) - min_count) * N / (count(word_a) * count(word_b)) > threshold , where N is the total vocabulary size.\n",
    "\n",
    "Gensim sets the default  threshold to 10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "# step 1: train the detector with\n",
    "phrases = models.phrases.Phrases(doc_list, min_count=2) # phrases have to occur at least two times\n",
    "# step 2: create a Phraser object to transform any sentence \n",
    "bigram = models.phrases.Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Exhibits', 'a', 'high', 'risk ', 'of', 'breast_cancer']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#little sanity check to see if it has worked: breast cancer should be detected as a collocation\n",
    "bigram['Exhibits', 'a', 'high', 'risk ' ,'of' , 'breast', 'cancer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Phraser object will then be used as a chained 'function' when creating the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec with CBOW\n",
    "\n",
    "We are using gensim's Word2Vec implementation and default parameter settings as described [here](https://radimrehurek.com/gensim/models/word2vec.html). \n",
    "\n",
    "We only modified the following parameters:\n",
    "- Words have to occur at least twice to be included in the vocabulary\n",
    "\n",
    "And we are detecting phrases as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec = models.Word2Vec(bigram[doc_list],min_count=2, workers=4)\n",
    "word2vec.save('our_word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 1: No stopword and chars seem to be removed\n",
    "\n",
    "This goes beyond the scope of our project work. Potentially, we come back here and search the gensim documentation for further implementation details.\n",
    "\n",
    "We also see that, certain n-grams that will be captured by fastText aren't part of the vocabulary (\"Can\"cer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True, False, True, True, True, False, False]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i in word2vec.wv for i in [ 'of', 'by', 'the','.',',','%','$','2', '23', '234','X','Can']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation 2: No (implicit) Lemmatization or Stemming has occured\n",
    "\n",
    "As of now, we'll live with this, but we should consider lemmatization/stemming eiter as a forther pre- or post-processing step (idea for post-processing: averaging over vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i in word2vec.wv for i in ['describe', 'described', 'describes', 'describing']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText\n",
    "\n",
    "FastText splits words into character n-grams of arbitray lenght (has to be specified as a range). It proceeds then same as Word2Vec (either Skipgram or CBOW architecture).\n",
    "\n",
    "##TODO: find paper that explains in more depth how skipgram works\n",
    "\n",
    "\n",
    "The advantage of fastText is that it makes predictions for out-of-vocabulary or misspelled terms, if they can be constructed from the character n-grams in the vocabulary.\n",
    "\n",
    "One major disadvantage of the existing fastText implementation is that word phrases such as “New York” are not being captured, further pre-processing would be necessary (Word2Vec does that for you).\n",
    "\n",
    "\n",
    "We are using the fastText implementation of gensim. \n",
    "Parameters are set as default , this also implies that we are using an CBOW representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText with CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.models.fasttext.FAST_VERSION > -1 # make sure that you are using Cython backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same procedure as above, will take substantially longer \n",
    "import gensim\n",
    "fasttext= gensim.models.FastText(doc_list, min_count= 2, min_n= 3, max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext.save('our_fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Beispiele, Similarities rausfinden, z.B: p-value/significant..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/d071503/Desktop/IR/irteamproject/embeddings\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
