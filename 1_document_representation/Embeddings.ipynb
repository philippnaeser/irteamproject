{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "### Please run this after running tfidf, we need those values to weight the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "#We need this line to find the collection_vocabulary.py here, else we cannot load the col.pkl object\n",
    "import sys\n",
    "sys.path.append('../0_Collection_and_Inverted_Index/')\n",
    "with open('../0_Collection_and_Inverted_Index/pickle/col.pkl', 'rb') as input:\n",
    "    col = pickle.load(input)\n",
    "inverted_index = pd.read_pickle('../0_Collection_and_Inverted_Index/pickle/inverted_index.pkl')\n",
    "\n",
    "tfidf=pd.read_pickle('pickle/tfidf.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further information on our approach to derive word embeddings can be found in embeddings_experiments.ipynb in this very same folder. Here, we only cover the two models we finally used for feature generation.\n",
    "\n",
    "It is important to understand the following first: There are basically two overlapping datasets provided by the authors. Firstly, 3633 docs represented as preprocessed BoWs, with each document being at least once (marginally) relevant for a given query. Secondly, 5731 raw docs, which are a superset of the 3633 BoW docs. This number is higher, since the authors crawled all these docs before categorizing them as (marginally) relevant. \n",
    "\n",
    "**Here, we use the 5731 raw docs to derive embeddings.**\n",
    "\n",
    "We used Gensim's FastText model class to derive the embeddings.\n",
    "\n",
    "We derived Word2Vec Embeddings with and without subword information, using the CBOW architecture.\n",
    "\n",
    "The Word2Vec approach is described in this [paper](https://arxiv.org/abs/1607.04606) on which is also based the [Gensim implementation.](https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "\n",
    "We sticked to Gensim's default parameters, with the following being the most important ones to mention:\n",
    "- embedding size: 100d dense vector\n",
    "- window size: 5\n",
    "- CBOW algorithm\n",
    "- training with hierarchical softmax\n",
    "- 5 iterations over the corpus\n",
    "\n",
    "We changed the following parameters:\n",
    "- minimum count of each word: 1\n",
    "- Word2Vec with subword information: \n",
    "    - min n-gram-size: 3\n",
    "    - max n-gram-size: 12\n",
    "\n",
    "We also considered includings bigrams, but decided against since in the precomputed BoW representation no bigrams were considered, and this would result in a mismatch between the vocabulary derived from the raw texts and the BoW vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5371"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# Gensim requires list of lists of Unicode 8 strings as an input. Since we have a small collection, \n",
    "# we are fine with loading everything into memory.\n",
    "import re\n",
    "doc_list= []\n",
    "with open('../nfcorpus/raw/doc_dump.txt', 'r', encoding='utf-8') as rf1:\n",
    "    for line in rf1:\n",
    "        l = re.sub(\"MED-.*\\t\", \"\",line).lower().strip('\\n').split()\n",
    "        doc_list.append(l) \n",
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.models.fasttext.FAST_VERSION > -1 # make sure that you are using Cython backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this to create a fasttext model of our documents\n",
    "#Name fasttest basically means Word2Vec with subword information\n",
    "fasttext= gensim.models.FastText(doc_list, min_count= 1, min_n= 3, max_n=12)\n",
    "fasttext.save('pickle/our_fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above, run this to compute the model, or run next cell to load it (if it exists on disk already)\n",
    "word2vec= gensim.models.FastText(doc_list, min_count= 1, word_ngrams=0)\n",
    "word2vec.save('pickle/our_fasttextword2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you already ran the upper part, you can load the results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save time, load the models, if they already exist.\n",
    "# This loads the whole models (not only the vectors).\n",
    "fasttext = gensim.models.FastText.load('pickle/our_fasttext') \n",
    "word2vec = gensim.models.FastText.load('pickle/our_fasttextword2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hort    [0.44308105, -1.5351962, 0.956442, 0.268957, 0...\n",
       "+        [-0.3428175, -0.34225887, 0.22346681, 0.418274...\n",
       "-        [-0.32701638, -0.20407611, 0.23003644, 0.11978...\n",
       "--a      [-0.06901108, -0.3414645, 0.15074101, 0.067805...\n",
       "--all    [0.033670183, -1.1816841, -0.099532396, 0.1932...\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec Embeddings with Subword Information, 100-d dense vector \n",
    "fasttext_embeddings_list=[]\n",
    "words_not_covered_in_fasttext=[]\n",
    "for word in inverted_index.index:\n",
    "    try:\n",
    "        fasttext_embeddings_list.append(fasttext.wv.get_vector(word))\n",
    "    except:\n",
    "        words_not_covered_in_fasttext.append(word)\n",
    "        fasttext_embeddings_list.append(np.zeros(100)) # for those 3 OOV we insert an array consisting of zeros\n",
    "fasttext_embeddings=pd.Series(fasttext_embeddings_list,index=inverted_index.index)\n",
    "fasttext_embeddings.to_pickle('pickle/fasttext_embeddings.pkl')\n",
    "fasttext_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hort    [-1.6446841, -0.78930265, 0.58323836, -0.39907...\n",
       "+        [-0.21700142, -0.18000382, -0.21318354, 0.6638...\n",
       "-        [-0.3929198, -0.25487423, -0.43005493, 0.54094...\n",
       "--a      [-0.3087517, -0.07765534, 0.098688155, -0.0462...\n",
       "--all    [0.09711393, -0.5630381, -0.5637208, -0.174061...\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec Embeddings, 100-d dense vector\n",
    "word2vec_embeddings_list=[]\n",
    "words_not_covered_in_word2vec=[]\n",
    "for word in inverted_index.index:\n",
    "    try:\n",
    "        word2vec_embeddings_list.append(word2vec.wv.get_vector(word))\n",
    "    except:\n",
    "        words_not_covered_in_word2vec.append(word)\n",
    "        word2vec_embeddings_list.append(np.zeros(100)) # for those 3 OOV we insert an array consisting of zeros\n",
    "word2vec_embeddings=pd.Series(word2vec_embeddings_list,index=inverted_index.index)\n",
    "word2vec_embeddings.to_pickle('pickle/word2vec_embeddings.pkl')\n",
    "word2vec_embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another shortcut here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embeddings = pd.read_pickle('pickle/fasttext_embeddings.pkl')\n",
    "word2vec_embeddings = pd.read_pickle('pickle/word2vec_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_embeddings(embeddings, tfidf_embed):\n",
    "    sum_of_tfidf_weights=tfidf_embed.sum(axis=0)#vector containing the normalizing constant for each doc\n",
    "    embeddings_dict={}\n",
    "    # we have to make use of the following workaround to avoid memory errors\n",
    "    # 1. calculate 100d embeddings vector for each doc/query and store it in dictionary\n",
    "    # 2. recreate a a dataframe containg the embeddings for all docs/queries from the dictionary\n",
    "    for doc in tfidf_embed.columns:\n",
    "        if doc not in embeddings_dict.keys():\n",
    "            embedding=(tfidf_embed[doc].mask(tfidf_embed[doc]!=0, other=(tfidf_embed[doc]*embeddings)).sum(axis=0))/sum_of_tfidf_weights[doc]\n",
    "            embeddings_dict[doc]=embedding\n",
    "        else:\n",
    "            print('calculated embeddings successfully and stored them in dictionary')\n",
    "    weighted_embedding = pd.DataFrame.from_dict(embeddings_dict)\n",
    "    return weighted_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_fasttext = get_weighted_embeddings(fasttext_embeddings, tfidf)\n",
    "\n",
    "#Let's save those again, as computing them might take a while\n",
    "documents_fasttext.to_pickle('pickle/documents_fasttext.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_word2vec= get_weighted_embeddings(word2vec_embeddings, tfidf)\n",
    "\n",
    "#Save them as well\n",
    "documents_word2vec.to_pickle('pickle/documents_word2vec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':{', 'nw', 'rq', 'w']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put this in report \n",
    "words_not_covered_in_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nw', 'rq', 'w']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put this in report \n",
    "words_not_covered_in_fasttexttext"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
