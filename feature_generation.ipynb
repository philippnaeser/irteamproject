{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TFIDF, BM25, UnigramLM\n",
    "\n",
    "## 1.Collection and Vocabulary\n",
    "\n",
    "Import *collection_vocabulary.py* and create an instance of the collection class. Its attributes are the collection itself, the vocabulary, and some descriptive summary statistics (e.g. vocabulary size, collection size, collection length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "1. Collection and Vocabulary\n",
    "2. Document Term Matrix\n",
    "3. Inverted Index\n",
    "4. TFIDF\n",
    "5. Unigram LM with J-M-Smoothing\n",
    "6. BM25\n",
    "7. Embeddings\n",
    "8. Queries\n",
    "\n",
    "We will fix and use the parameters of the different models as discussed in the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collection_vocabulary import Collection\n",
    "col=Collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Term Matrix\n",
    "The document term matrix is obtained as a lists of lists, and then converted to a Pandas dataframe, which is stored to disk to facilitate debugging, and further experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_term_matrix=[]\n",
    "for doc in col.collection:\n",
    "    tf_vector =[]\n",
    "    for word in col.vocabulary:\n",
    "        n= col.collection[doc].count(word)\n",
    "        tf_vector.append(n)\n",
    "    doc_term_matrix.append(tf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "doc_term_matrix= pd.DataFrame(data=doc_term_matrix,index= col.collection.keys(),columns=col.vocabulary)\n",
    "doc_term_matrix.to_pickle('doc_term_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'hort</th>\n",
       "      <th>+</th>\n",
       "      <th>-</th>\n",
       "      <th>--a</th>\n",
       "      <th>--all</th>\n",
       "      <th>--have</th>\n",
       "      <th>--mainly</th>\n",
       "      <th>--of</th>\n",
       "      <th>--showed</th>\n",
       "      <th>--the</th>\n",
       "      <th>...</th>\n",
       "      <th>zooplankton</th>\n",
       "      <th>zoxazolamine</th>\n",
       "      <th>zr</th>\n",
       "      <th>zu</th>\n",
       "      <th>zuccarini</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zugesetztem</th>\n",
       "      <th>zusatzstoffe-online</th>\n",
       "      <th>zygote</th>\n",
       "      <th>zymography</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MED-10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-118</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29052 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         'hort  +  -  --a  --all  --have  --mainly  --of  --showed  --the  \\\n",
       "MED-10       0  0  0    0      0       0         0     0         0      0   \n",
       "MED-14       0  0  0    0      0       0         0     0         0      0   \n",
       "MED-118      0  0  0    0      0       0         0     0         0      0   \n",
       "\n",
       "            ...      zooplankton  zoxazolamine  zr  zu  zuccarini  zucchini  \\\n",
       "MED-10      ...                0             0   0   0          0         0   \n",
       "MED-14      ...                0             0   0   0          0         0   \n",
       "MED-118     ...                0             0   0   0          0         0   \n",
       "\n",
       "         zugesetztem  zusatzstoffe-online  zygote  zymography  \n",
       "MED-10             0                    0       0           0  \n",
       "MED-14             0                    0       0           0  \n",
       "MED-118            0                    0       0           0  \n",
       "\n",
       "[3 rows x 29052 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix.head(3) # this is how the doc term matrix looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3633, 29052)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check: should have dimensions 3633*29052\n",
    "doc_term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3633.000000\n",
       "mean      146.204789\n",
       "std        53.995711\n",
       "min        11.000000\n",
       "25%       114.000000\n",
       "50%       148.000000\n",
       "75%       174.000000\n",
       "max       939.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some summary stats for our project report and a sanity check that would reveal any empty docs\n",
    "doc_term_matrix.sum(axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inverted Index\n",
    "\n",
    "The inverted index is our unified (and in practice memory-efficient) way of representing the document term matrix that we will use in the remainder of this project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good point to start off the actual analysis and calculate different retrieval models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverted_index= doc_term_matrix.transpose()\n",
    "inverted_index.to_pickle('inverted_index.pkl') # use later for embeddings, queries, ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check 1\n",
    "# each term should occur at least once (implied by the way we construct the index), hence min>=1\n",
    "inverted_index.sum(axis=1).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=(inverted_index>0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_idf=(col.collection_size/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zucchini               3633.0\n",
       "zugesetztem            3633.0\n",
       "zusatzstoffe-online    3633.0\n",
       "zygote                 3633.0\n",
       "zymography             1816.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_idf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zucchini               3.560265\n",
       "zugesetztem            3.560265\n",
       "zusatzstoffe-online    3.560265\n",
       "zygote                 3.560265\n",
       "zymography             3.259235\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf= np.log10(raw_idf) #aka log of raw_idf\n",
    "idf.to_pickle('idf.pkl') #use the global idf scores for queries later\n",
    "idf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: max tf score should be equal to number of docs in collection...\n",
    "raw_idf.max().max()==3633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5602653978627146"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: ... and max idf score should be substantially lower\n",
    "idf.max().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF\n",
    "Raw term frequency is what we obtain when we look columnwise at the  *inverted_index* dataframe.\n",
    "As discussed in the lecture, we will normalize this frequency by dividing with the raw frequency of the most frequent term in each document. Next, we then take the logarithm (any logarithm will do the job) since we assume that relevance does not increase linearly with term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philipp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# nominator part\n",
    "nominator=inverted_index.mask(inverted_index!=0,other=(np.log10(inverted_index)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29052, 3633)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nominator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# denominator part\n",
    "most_frequent_term=inverted_index.max(axis=0) # determine most frequent term in each doc\n",
    "denominator= np.log10(most_frequent_term)\n",
    "denominator+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3633,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denominator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check, there shouldn't be any zeros\n",
    "denominator.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check, there shouldn't be any zeros\n",
    "nominator.min().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf\n",
    "tf=nominator.div(denominator, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "Bringing the pieces together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf= tf.mul(idf, axis=0) # we multiply the tf scores in every doc with the corresponding idf scores\n",
    "tfidf.to_pickle('tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29052, 3633)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Unigram LM with Jelinek-Mercer-Smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Language Model\n",
    "We want to find out how likely each word is if we look at the whole corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_LM=inverted_index.sum(axis=1)/col.collection_length # equal: inverted_index.sum(axis=1)/inverted_index.sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Language Models\n",
    "We want to obtain a language model for each document in the collection, therefore we look at the columns of the *inverted_index* dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local_LMs=inverted_index/inverted_index.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3633.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check: Probabilities should sum columnwise to 1, and adding all columns should yield the collection size (3633)\n",
    "local_LMs.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram LM with J-M-Smoothing\n",
    "As introduced in the lecture, this smoothing scheme assigns equal weights to the global and local LMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_LM= (local_LMs.apply(lambda x: x+ global_LM)).apply(lambda x: x/2)# same as multiplying both by 0.5 and adding them\n",
    "unigram_LM.to_pickle('unigramLM.pkl')#writing Unigram-LM to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3633.0000000012674"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check: probabilities in every doc should sum up to one and all docs should sum up tp 3633\n",
    "unigram_LM.sum().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check: we don't want to have any negative values \n",
    "unigram_LM.min().min()<0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check: we don't want to have any zeros (since we are smoothing)\n",
    "unigram_LM.isnull().sum(axis=1).sum()==0 # we check whether therer are no zeros > True intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nomitted: operating in log-space to avoid numerical instability\\nglobal_LM_log_space=np.log(global_LM)\\nlocal_LMs_log_space=local_LMs.applymap(lambda x: np.log(x, out=np.zeros_like(inverted_index.as_matrix),where=x!=0))\\n\\n '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "omitted: operating in log-space to avoid numerical instability\n",
    "global_LM_log_space=np.log(global_LM)\n",
    "local_LMs_log_space=local_LMs.applymap(lambda x: np.log(x, out=np.zeros_like(inverted_index.as_matrix),where=x!=0))\n",
    "\n",
    " '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. BIM 25\n",
    "Let's approach BIM25 step by step, which means modeling the BIM and then gradually extending it. \n",
    "We start from the naive assumption that we do not have any relevance feedbacks.\n",
    "\n",
    "\n",
    "### BIM \n",
    "This simplification results in the following formula we want to compute:\n",
    "w_t= log(0.5 * N/N_t)\n",
    "\n",
    "N_t signifies in how many documents a term appears. This is what we already calucalted as the 'raw' document frequency in the TFIDF-model above. \n",
    "What we are basically doing is multiplying the raw inverse document frequency by 0.5 and then taking the logarithm.\n",
    "\n",
    "Note: This can (and is intended to) produce negative values for words occuring in almost every document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hort    3.259235\n",
       "+        2.782114\n",
       "-        3.259235\n",
       "--a      3.259235\n",
       "--all    3.259235\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BIM= np.log10(raw_idf*0.5) # raw idf calculated above in tfidf\n",
    "BIM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observation: in BIM 25 weights may actually become negative - we have four negative weights\n",
    "sum(BIM<0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM 25\n",
    "Let's focus on the weighting part and then multiply these weights with the BIM weights from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters as presented in the lecture\n",
    "k=1.5\n",
    "b=0.25\n",
    "document_lenght= inverted_index.sum()\n",
    "average_document_length= col.collection_length/col.collection_size # 146.20478943022295 TODO: include in project report\n",
    "doc_len_div_by_avg_doc_len= document_lenght/average_document_length\n",
    "#sanity check, should yield 3633\n",
    "doc_len_div_by_avg_doc_len.sum() == 3633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29052, 3633)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighting_bim25_nominator= inverted_index*k*(k+1)\n",
    "weighting_bim25_nominator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29052, 3633)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the denominator is the tricky part since we have to add scalars and a vector to each column in the inverted index at the same time\n",
    "weighting_bim25_denominator=inverted_index.add((doc_len_div_by_avg_doc_len*k*b), axis=1)+(k*(1-b))\n",
    "weighting_bim25_denominator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merging nominator and denominator\n",
    "weighting_bim25= weighting_bim25_nominator.div(weighting_bim25_denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29052, 3633)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check: 29052, 3633 ?\n",
    "weighting_bim25.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the weights, and the vanilla BIM from above, we can now construct BIM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BIM25=weighting_bim25.mul(BIM, axis=0)\n",
    "BIM25.to_pickle('BIM25.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further information to the embeddings can be found in Word Embeddings.ipynb, here we only use the parts for feature generation.\n",
    "\n",
    "We use two versions of embeddings here, fasttext and fasttext.word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5371"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# Gensim requires list of lists of Unicode 8 strings as an input. Since we have a small collection, \n",
    "# we are fine with loading everything into memory.\n",
    "import re\n",
    "doc_list= []\n",
    "with open('./nfcorpus/raw/doc_dump.txt', 'r', encoding='utf-8') as rf1:\n",
    "    for line in rf1:\n",
    "        l = re.sub(\"MED-.*\\t\", \"\",line).lower().strip('\\n').split()\n",
    "        doc_list.append(l) \n",
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.models.fasttext.FAST_VERSION > -1 # make sure that you are using Cython backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run this to create a fasttext model of our documents\n",
    "#fasttext= gensim.models.FastText(bigram[doc_list], min_count= 1, min_n= 3, max_n=12)\n",
    "fasttext= gensim.models.FastText(doc_list, min_count= 1, min_n= 3, max_n=12)\n",
    "fasttext.save('our_fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Same as above, run this to compute the model, or run next cell to load it (if it exists on disk already)\n",
    "word2vec= gensim.models.FastText(doc_list, min_count= 1, word_ngrams=0)\n",
    "word2vec.save('our_fasttextword2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that this first step is only necessary if you want a shortcut, and already computed the documents some time before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philipp\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#To speed things up, load the pickle files. If you just ran the whole script, you do not need this step\n",
    "#Note: pkl files are excluded from git for being to large, so you have to run the whole script once\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "tfidf = pd.read_pickle('tfidf.pkl')\n",
    "BIM25 = pd.read_pickle('BIM25.pkl')\n",
    "unigram_LM = pd.read_pickle('unigramLM.pkl')\n",
    "idf = pd.read_pickle('idf.pkl')\n",
    "from collection_vocabulary import Collection\n",
    "col=Collection()\n",
    "# this loads the whole model, (not only the vectors)\n",
    "fasttext = gensim.models.FastText.load('our_fasttext')\n",
    "word2vec = gensim.models.FastText.load('our_fasttextword2vec')\n",
    "inverted_index = pd.read_pickle('inverted_index.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample single term queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the same single term query  - \"cancer\". And compare the results of the three retrieval models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MED-3718    0.695754\n",
       "MED-2322    0.695754\n",
       "MED-5063    0.695754\n",
       "MED-2137    0.695754\n",
       "MED-4096    0.695754\n",
       "MED-4643    0.695754\n",
       "MED-4117    0.695754\n",
       "MED-1414    0.695754\n",
       "MED-3551    0.695754\n",
       "MED-3550    0.695754\n",
       "Name: cancer, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TFIDF\n",
    "a= tfidf.loc['cancer'].sort_values(ascending=False).head(10) # if you transpose you can directly select by the index term  > tf.transpose().cancer\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MED-3703    0.081339\n",
       "MED-2137    0.061650\n",
       "MED-2174    0.057772\n",
       "MED-4391    0.052210\n",
       "MED-890     0.048745\n",
       "MED-5184    0.048281\n",
       "MED-3551    0.047909\n",
       "MED-3555    0.047347\n",
       "MED-2258    0.045462\n",
       "MED-3699    0.044962\n",
       "Name: cancer, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram LM\n",
    "b= unigram_LM.loc['cancer'].sort_values(ascending=False).head(10)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MED-3703    0.917499\n",
       "MED-1721    0.895649\n",
       "MED-2760    0.894057\n",
       "MED-3699    0.892689\n",
       "MED-3555    0.884759\n",
       "MED-14      0.881640\n",
       "MED-4928    0.880848\n",
       "MED-5353    0.877033\n",
       "MED-4050    0.876367\n",
       "MED-4785    0.876234\n",
       "Name: cancer, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= BIM25.loc['cancer'].sort_values(ascending=False).head(10)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, there is very little overlap in the top 10 retrieved documents. Only the top-ranked doc of the probabilisitic ranking models matches.\n",
    "\n",
    "Now, let's get the query representations and compute the scores for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here the query part really starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1              3\n",
      "count  139350.0  139350.000000\n",
      "mean        0.0       1.824212\n",
      "std         0.0       0.454204\n",
      "min         0.0       1.000000\n",
      "25%         0.0       2.000000\n",
      "50%         0.0       2.000000\n",
      "75%         0.0       2.000000\n",
      "max         0.0       3.000000\n",
      "             1             3\n",
      "count  15820.0  15820.000000\n",
      "mean       0.0      1.816056\n",
      "std        0.0      0.472168\n",
      "min        0.0      1.000000\n",
      "25%        0.0      2.000000\n",
      "50%        0.0      2.000000\n",
      "75%        0.0      2.000000\n",
      "max        0.0      3.000000\n"
     ]
    }
   ],
   "source": [
    "#Now to get the queries\n",
    "train_queries = pd.read_csv('nfcorpus/train.all.queries', sep='\\t', header=None)\n",
    "train_queries.columns = ['id', 'text']\n",
    "dev_queries = pd.read_csv('nfcorpus/dev.all.queries', sep='\\t', header=None)\n",
    "dev_queries.columns = ['id', 'text']\n",
    "test_queries = pd.read_csv('nfcorpus/test.all.queries', sep='\\t', header=None)\n",
    "test_queries.columns = ['id', 'text']\n",
    "\n",
    "#And the relevance scores given\n",
    "train_rel = pd.read_csv('nfcorpus/train.3-2-1.qrel', sep='\\t', header=None)\n",
    "print(train_rel.describe())\n",
    "test_rel = pd.read_csv('nfcorpus/test.3-2-1.qrel', sep='\\t', header=None)\n",
    "print(test_rel.describe())\n",
    "dev_rel = pd.read_csv('nfcorpus/dev.3-2-1.qrel', sep='\\t', header=None)\n",
    "#As we can see, column 1 is always 0, so drop it\n",
    "train_rel = train_rel.drop([1], axis=1)\n",
    "dev_rel = dev_rel.drop([1], axis=1)\n",
    "test_rel = test_rel.drop([1], axis=1)\n",
    "train_rel.columns = ['qid', 'docid', 'rel']\n",
    "dev_rel.columns = ['qid', 'docid', 'rel']\n",
    "test_rel.columns = ['qid', 'docid', 'rel']\n",
    "\n",
    "#The corpus also divides documents into train, dev and test, so we need to stick to that as well\n",
    "#(in order to get comparable results)\n",
    "train_docs = pd.read_csv('nfcorpus/train.docs', sep='\\t', header=None)\n",
    "train_docs.columns = ['id', 'text']\n",
    "dev_docs = pd.read_csv('nfcorpus/dev.docs', sep='\\t', header=None)\n",
    "dev_docs.columns = ['id', 'text']\n",
    "test_docs = pd.read_csv('nfcorpus/test.docs', sep='\\t', header=None)\n",
    "test_docs.columns = ['id', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you can skip this if you already did it once, just start loading the matrices from pkl files\n",
    "def get_query_term_matrix(queries, col):\n",
    "    query_term_matrix = []\n",
    "    for query in queries.itertuples():\n",
    "        tf_vector =[]\n",
    "        for word in col.vocabulary:\n",
    "            n= query.text.count(word)\n",
    "            tf_vector.append(n)\n",
    "        query_term_matrix.append(tf_vector)\n",
    "    return pd.DataFrame(data=query_term_matrix,index=queries.id,columns=col.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's compute the term_matrix for our query texts\n",
    "train_matrix = get_query_term_matrix(train_queries, col)\n",
    "dev_matrix = get_query_term_matrix(dev_queries, col)\n",
    "test_matrix = get_query_term_matrix(test_queries, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'hort</th>\n",
       "      <th>+</th>\n",
       "      <th>-</th>\n",
       "      <th>--a</th>\n",
       "      <th>--all</th>\n",
       "      <th>--have</th>\n",
       "      <th>--mainly</th>\n",
       "      <th>--of</th>\n",
       "      <th>--showed</th>\n",
       "      <th>--the</th>\n",
       "      <th>...</th>\n",
       "      <th>zooplankton</th>\n",
       "      <th>zoxazolamine</th>\n",
       "      <th>zr</th>\n",
       "      <th>zu</th>\n",
       "      <th>zuccarini</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zugesetztem</th>\n",
       "      <th>zusatzstoffe-online</th>\n",
       "      <th>zygote</th>\n",
       "      <th>zymography</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PLAIN-1008</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAIN-1018</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAIN-102</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAIN-1028</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAIN-1039</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29052 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            'hort  +   -  --a  --all  --have  --mainly  --of  --showed  --the  \\\n",
       "id                                                                              \n",
       "PLAIN-1008      0  0   4    0      0       0         0     0         0      0   \n",
       "PLAIN-1018      0  0  11    0      0       0         0     0         0      0   \n",
       "PLAIN-102       0  1  54    0      0       0         0     0         0      0   \n",
       "PLAIN-1028      0  0   5    0      0       0         0     0         0      0   \n",
       "PLAIN-1039      0  0   4    0      0       0         0     0         0      0   \n",
       "\n",
       "               ...      zooplankton  zoxazolamine  zr  zu  zuccarini  \\\n",
       "id             ...                                                     \n",
       "PLAIN-1008     ...                0             0   0   0          0   \n",
       "PLAIN-1018     ...                0             0   0   0          0   \n",
       "PLAIN-102      ...                0             0   0   0          0   \n",
       "PLAIN-1028     ...                0             0   0   0          0   \n",
       "PLAIN-1039     ...                0             0   0   0          0   \n",
       "\n",
       "            zucchini  zugesetztem  zusatzstoffe-online  zygote  zymography  \n",
       "id                                                                          \n",
       "PLAIN-1008         0            0                    0       0           0  \n",
       "PLAIN-1018         0            0                    0       0           0  \n",
       "PLAIN-102          0            0                    0       0           0  \n",
       "PLAIN-1028         0            0                    0       0           0  \n",
       "PLAIN-1039         0            0                    0       0           0  \n",
       "\n",
       "[5 rows x 29052 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Another speed up, save the matrices\n",
    "train_matrix.to_pickle('queries/train_matrix.pkl')\n",
    "dev_matrix.to_pickle('queries/dev_matrix.pkl')\n",
    "train_matrix.to_pickle('queries/test_matrix.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you already ran this once, you can simply load the matrices instead of computing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_matrix = pd.read_pickle('queries/train_matrix.pkl')\n",
    "dev_matrix = pd.read_pickle('queries/dev_matrix.pkl')\n",
    "test_matrix = pd.read_pickle('queries/test_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As seen in the tf idf computation of the documents, we will use an inverted matrix\n",
    "train_matrix_inverted = train_matrix.transpose()\n",
    "dev_matrix_inverted = dev_matrix.transpose()\n",
    "test_matrix_inverted = test_matrix.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_query_tfidf(inv_query_matrix, idf):\n",
    "    #TF\n",
    "    # nominator part\n",
    "    nominator=inv_query_matrix.mask(inv_query_matrix!=0,other=(np.log10(inv_query_matrix)+1))\n",
    "    # denominator part\n",
    "    most_frequent_term=inv_query_matrix.max(axis=0) # determine most frequent term in each query\n",
    "    denominator= np.log10(most_frequent_term)\n",
    "    denominator+=1\n",
    "    tf=nominator.div(denominator, axis=1)\n",
    "    tfidf_query= tf.mul(idf, axis=0) # we multiply the tf scores in every query with the corresponding idf scores\n",
    "    return tfidf_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philipp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log10\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Now, let's get the tfidf scores for each query, please ignore the error\n",
    "train_tfidf = compute_query_tfidf(train_matrix_inverted, idf)\n",
    "dev_tfidf = compute_query_tfidf(dev_matrix_inverted, idf)\n",
    "test_tfidf = compute_query_tfidf(test_matrix_inverted, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save those as pkl as well, because that also takes quite a while to compute\n",
    "train_tfidf.to_pickle('queries/train_tfidf.pkl')\n",
    "dev_tfidf.to_pickle('queries/dev_tfidf.pkl')\n",
    "test_tfidf.to_pickle('queries/test_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again, load it to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Or load them, if they already exist\n",
    "train_tfidf = pd.read_pickle('queries/train_tfidf.pkl')\n",
    "dev_tfidf = pd.read_pickle('queries/dev_tfidf.pkl')\n",
    "test_tfidf = pd.read_pickle('queries/test_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>PLAIN-1008</th>\n",
       "      <th>PLAIN-1018</th>\n",
       "      <th>PLAIN-102</th>\n",
       "      <th>PLAIN-1028</th>\n",
       "      <th>PLAIN-1039</th>\n",
       "      <th>PLAIN-1050</th>\n",
       "      <th>PLAIN-1066</th>\n",
       "      <th>PLAIN-1078</th>\n",
       "      <th>PLAIN-1088</th>\n",
       "      <th>PLAIN-1098</th>\n",
       "      <th>...</th>\n",
       "      <th>PLAIN-91</th>\n",
       "      <th>PLAIN-913</th>\n",
       "      <th>PLAIN-924</th>\n",
       "      <th>PLAIN-934</th>\n",
       "      <th>PLAIN-946</th>\n",
       "      <th>PLAIN-956</th>\n",
       "      <th>PLAIN-966</th>\n",
       "      <th>PLAIN-977</th>\n",
       "      <th>PLAIN-987</th>\n",
       "      <th>PLAIN-997</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'hort</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>2.529077</td>\n",
       "      <td>2.433363</td>\n",
       "      <td>2.366582</td>\n",
       "      <td>2.628729</td>\n",
       "      <td>2.621103</td>\n",
       "      <td>2.5047</td>\n",
       "      <td>2.604743</td>\n",
       "      <td>2.726138</td>\n",
       "      <td>2.44186</td>\n",
       "      <td>2.918655</td>\n",
       "      <td>...</td>\n",
       "      <td>1.971896</td>\n",
       "      <td>1.79211</td>\n",
       "      <td>2.682064</td>\n",
       "      <td>1.856777</td>\n",
       "      <td>2.744308</td>\n",
       "      <td>2.022176</td>\n",
       "      <td>2.561169</td>\n",
       "      <td>2.587771</td>\n",
       "      <td>2.434983</td>\n",
       "      <td>2.680533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--a</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--all</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id     PLAIN-1008  PLAIN-1018  PLAIN-102  PLAIN-1028  PLAIN-1039  PLAIN-1050  \\\n",
       "'hort    0.000000    0.000000   0.000000    0.000000    0.000000      0.0000   \n",
       "+        0.000000    0.000000   0.750049    0.000000    0.000000      0.0000   \n",
       "-        2.529077    2.433363   2.366582    2.628729    2.621103      2.5047   \n",
       "--a      0.000000    0.000000   0.000000    0.000000    0.000000      0.0000   \n",
       "--all    0.000000    0.000000   0.000000    0.000000    0.000000      0.0000   \n",
       "\n",
       "id     PLAIN-1066  PLAIN-1078  PLAIN-1088  PLAIN-1098    ...      PLAIN-91  \\\n",
       "'hort    0.000000    0.000000     0.00000    0.000000    ...      0.000000   \n",
       "+        0.000000    0.000000     0.00000    0.000000    ...      0.000000   \n",
       "-        2.604743    2.726138     2.44186    2.918655    ...      1.971896   \n",
       "--a      0.000000    0.000000     0.00000    0.000000    ...      0.000000   \n",
       "--all    0.000000    0.000000     0.00000    0.000000    ...      0.000000   \n",
       "\n",
       "id     PLAIN-913  PLAIN-924  PLAIN-934  PLAIN-946  PLAIN-956  PLAIN-966  \\\n",
       "'hort    0.00000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "+        0.00000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "-        1.79211   2.682064   1.856777   2.744308   2.022176   2.561169   \n",
       "--a      0.00000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "--all    0.00000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "id     PLAIN-977  PLAIN-987  PLAIN-997  \n",
       "'hort   0.000000   0.000000   0.000000  \n",
       "+       0.000000   0.000000   0.000000  \n",
       "-       2.587771   2.434983   2.680533  \n",
       "--a     0.000000   0.000000   0.000000  \n",
       "--all   0.000000   0.000000   0.000000  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hort    [0.727683, -0.649744, 0.607231, -0.566998, 0.4...\n",
       "+        [-0.315303, -0.304781, 0.192003, 0.625403, 0.4...\n",
       "-        [-0.414824, -0.482533, 0.302804, 0.319794, 0.4...\n",
       "--a      [-0.0802856, -0.321893, 0.167339, 0.024268, 0....\n",
       "--all    [0.520891, -0.499456, -0.229592, 0.0148075, 0....\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_embeddings_list=[]\n",
    "words_not_covered_in_fasttext=[]\n",
    "for word in inverted_index.index:\n",
    "    try:\n",
    "        fasttext_embeddings_list.append(fasttext.wv.get_vector(word))\n",
    "    except:\n",
    "        words_not_covered_in_fasttext.append(word)\n",
    "        fasttext_embeddings_list.append(np.zeros(100)) # for those 3 OOV we insert an array consisting of zeros\n",
    "fasttext_embeddings=pd.Series(fasttext_embeddings_list,index=inverted_index.index)\n",
    "fasttext_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hort    [-0.713965, -0.403524, 0.314923, -1.19988, -0....\n",
       "+        [-0.605538, -0.46913, -0.150712, 1.10127, -0.1...\n",
       "-        [-0.904962, -0.500117, -0.138728, 1.11224, -0....\n",
       "--a      [-0.277904, -0.0720914, 0.105588, 0.0845153, -...\n",
       "--all    [0.328376, -0.181311, -0.67753, -1.00159, 0.55...\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec Embeddings, 100-d dense vector\n",
    "word2vec_embeddings_list=[]\n",
    "words_not_covered_in_word2vec=[]\n",
    "for word in inverted_index.index:\n",
    "    try:\n",
    "        word2vec_embeddings_list.append(word2vec.wv.get_vector(word))\n",
    "    except:\n",
    "        words_not_covered_in_word2vec.append(word)\n",
    "        word2vec_embeddings_list.append(np.zeros(100)) # for those 3 OOV we insert an array consisting of zeros\n",
    "word2vec_embeddings=pd.Series(word2vec_embeddings_list,index=inverted_index.index)\n",
    "word2vec_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_embeddings(embeddings, tfidf_embed):\n",
    "    sum_of_tfidf_weights=tfidf_embed.sum(axis=0)#vector containing the normalizing constant for each doc\n",
    "    weighted_embeddings=tfidf_embed.mask(tfidf_embed!=0, other=(tfidf_embed*embeddings).div(sum_of_tfidf_weights))\n",
    "    print('done')\n",
    "    return weighted_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_fasttext = get_weighted_embeddings(fasttext_embeddings, tfidf)\n",
    "train_queries_fasttext = get_weighted_embeddings(fasttext_embeddings, train_tfidf)\n",
    "dev_queries_fasttext = get_weighted_embeddings(fasttext_embeddings, dev_tfidf)\n",
    "test_queries_fasttext = get_weighted_embeddings(fasttext_embeddings, test_tfidf)\n",
    "#Let's save those again, as computing them might take a while\n",
    "documents_fasttext.to_pickle('documents_fasttext.pkl')\n",
    "train_queries_fasttext.to_pickle('queries/train_queries_fasttext.pkl')\n",
    "dev_queries_fasttext.to_pickle('queries/dev_queries_fasttext.pkl')\n",
    "test_queries_fasttext.to_pickle('queries/test_queries_fasttext.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_word2vec= get_weighted_embeddings(word2vec_embeddings, tfidf)\n",
    "train_queries_word2vec = get_weighted_embeddings(word2vec_embeddings, train_tfidf)\n",
    "dev_queries_word2vec = get_weighted_embeddings(word2vec_embeddings, dev_tfidf)\n",
    "test_queries_word2vec = get_weighted_embeddings(word2vec_embeddings, test_tfidf)\n",
    "#Save them as well\n",
    "documents_word2vec.to_pickle('documents_word2vec.pkl')\n",
    "train_queries_word2vec.to_pickle('queries/train_queries_word2vec.pkl')\n",
    "dev_queries_word2vec.to_pickle('queries/dev_queries_word2vec.pkl')\n",
    "test_queries_word2vec.to_pickle('queries/test_queries_word2vec.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the weighted embeddings, if you precomputed them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_fasttext = pd.read_pickle('documents_fasttext.pkl')\n",
    "train_queries_fasttext = pd.read_pickle('queries/train_queries_fasttext.pkl')\n",
    "dev_queries_fasttext = pd.read_pickle('queries/dev_queries_fasttext.pkl')\n",
    "test_queries_fasttext = pd.read_pickle('queries/test_queries_fasttext.pkl')\n",
    "documents_word2vec = pd.read_pickle('documents_word2vec.pkl')\n",
    "train_queries_word2vec = pd.read_pickle('queries/train_queries_word2vec.pkl')\n",
    "dev_queries_word2vec = pd.read_pickle('queries/dev_queries_word2vec.pkl')\n",
    "test_queries_word2vec = pd.read_pickle('queries/test_queries_word2vec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function assumes that you either ran the whole code or ran the shortcut step and the code from there on\n",
    "def compute_scores(queries, documents, rel, queries_tfidf):#, queries_fasttext, queries_word2vec):\n",
    "    #Get the documents defined in the nfcorpous\n",
    "    doc_keys = documents.id\n",
    "    tfidf_part = tfidf.loc[:, doc_keys]\n",
    "    BIM25_part = BIM25.loc[:, doc_keys]\n",
    "    unigram_LM_part = unigram_LM.loc[:, doc_keys]\n",
    "    #Get the cosine between queries and docs (much faster than inside the loop)\n",
    "    cosine = cosine_similarity(queries_tfidf, tfidf_part.transpose())\n",
    "    list_of_df = []\n",
    "    query_keys = queries['id']\n",
    "    print('Computing', len(query_keys), 'queries on', len(doc_keys), 'documents')\n",
    "    i = 0\n",
    "    for key in query_keys:\n",
    "        text = str(queries.loc[queries['id'] == key].text)\n",
    "        tfidf_scores = tfidf_part.loc[text.split()].sum()\n",
    "        bim25_scores = BIM25_part.loc[text.split()].sum()\n",
    "        unigram_scores = unigram_LM_part.loc[text.split()].product()\n",
    "        cosine_scores = cosine[i]\n",
    "        total = pd.DataFrame()\n",
    "        total['tfidf'] = tfidf_scores\n",
    "        total['bim25'] = bim25_scores\n",
    "        total['unigram'] = unigram_scores\n",
    "        total['cosine'] = cosine_scores\n",
    "        total['qid'] = key.replace('PLAIN-', '')\n",
    "        #Rel only contains 1 and 2, everything that is not in there is set to 0\n",
    "        total['rel'] = 0\n",
    "        rel_temp = rel.loc[(rel['qid'] == key)]\n",
    "        for row in rel_temp.itertuples():\n",
    "            total.at[row.docid, 'rel'] = row.rel\n",
    "        total.set_index(np.arange(len(doc_keys)))\n",
    "        total.rename(columns={'': 'docid'}, inplace=True)\n",
    "        list_of_df.append(total)\n",
    "        i+=1\n",
    "        if (i%100 == 0):\n",
    "            print(i, 'queries computed')\n",
    "    scores = pd.concat(list_of_df)\n",
    "    print(i, 'queries computed')\n",
    "    #Sanity check: should be same\n",
    "    print(len(scores))\n",
    "    print(len(doc_keys)*len(query_keys))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "def cosine_similarity(query, docs):\n",
    "    cos_similarity = 1-distance.cdist(query, docs, metric='cosine')\n",
    "    return cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 2594 queries on 3612 documents\n",
      "100 queries computed\n",
      "200 queries computed\n",
      "300 queries computed\n",
      "400 queries computed\n",
      "500 queries computed\n",
      "600 queries computed\n",
      "700 queries computed\n",
      "800 queries computed\n",
      "900 queries computed\n",
      "1000 queries computed\n",
      "1100 queries computed\n",
      "1200 queries computed\n",
      "1300 queries computed\n",
      "1400 queries computed\n",
      "1500 queries computed\n",
      "1600 queries computed\n",
      "1700 queries computed\n",
      "1800 queries computed\n",
      "1900 queries computed\n",
      "2000 queries computed\n",
      "2100 queries computed\n",
      "2200 queries computed\n",
      "2300 queries computed\n",
      "2400 queries computed\n",
      "2500 queries computed\n",
      "2594 queries computed\n",
      "9369528\n",
      "9369528\n",
      "Computing 325 queries on 3193 documents\n",
      "100 queries computed\n",
      "200 queries computed\n",
      "300 queries computed\n",
      "325 queries computed\n",
      "1037725\n",
      "1037725\n",
      "Computing 325 queries on 3162 documents\n",
      "100 queries computed\n",
      "200 queries computed\n",
      "300 queries computed\n",
      "325 queries computed\n",
      "1027650\n",
      "1027650\n"
     ]
    }
   ],
   "source": [
    "train_scores = compute_scores(train_queries, train_docs, train_rel, train_tfidf.transpose())\n",
    "dev_scores = compute_scores(dev_queries, dev_docs, dev_rel, dev_tfidf.transpose())\n",
    "test_scores = compute_scores(test_queries, test_docs, test_rel, test_tfidf.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "      <th>bim25</th>\n",
       "      <th>unigram</th>\n",
       "      <th>cosine</th>\n",
       "      <th>qid</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MED-10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.015284</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-118</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.024806</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-301</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-306</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-329</th>\n",
       "      <td>0.727490</td>\n",
       "      <td>0.931327</td>\n",
       "      <td>1.404000e-22</td>\n",
       "      <td>0.016845</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-330</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.014549</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-332</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.027112</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-334</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.022925</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-335</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.020251</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-398</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.013594</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-557</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.013787</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-666</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.011882</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-691</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-692</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-702</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.019939</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-706</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.013718</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-707</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.037240</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-708</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-709</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.020028</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-711</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.018396</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-712</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.013854</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-713</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-714</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.032292</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-716</th>\n",
       "      <td>0.632461</td>\n",
       "      <td>0.975177</td>\n",
       "      <td>1.890716e-22</td>\n",
       "      <td>0.020736</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-717</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-718</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.011552</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-719</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.013662</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-720</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-721</th>\n",
       "      <td>0.617991</td>\n",
       "      <td>0.953745</td>\n",
       "      <td>1.616350e-22</td>\n",
       "      <td>0.022873</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5342</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.008694</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5343</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5344</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.030737</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5345</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5346</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5347</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5348</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5349</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5350</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5351</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5352</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.018953</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5353</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5354</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5355</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5356</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5357</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5358</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5359</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5360</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5361</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5362</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5363</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.010336</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5364</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.018986</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5365</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5366</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.008911</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5367</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5368</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5369</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5370</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-5371</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.798837e-33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9369528 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             tfidf     bim25       unigram    cosine  qid  rel\n",
       "MED-10    0.000000  0.000000  2.314569e-23  0.015284   10    0\n",
       "MED-14    0.000000  0.000000  2.314569e-23  0.010135   10    0\n",
       "MED-118   0.000000  0.000000  2.314569e-23  0.024806   10    0\n",
       "MED-301   0.000000  0.000000  2.314569e-23  0.019779   10    0\n",
       "MED-306   0.000000  0.000000  2.314569e-23  0.030273   10    0\n",
       "MED-329   0.727490  0.931327  1.404000e-22  0.016845   10    0\n",
       "MED-330   0.000000  0.000000  2.314569e-23  0.014549   10    0\n",
       "MED-332   0.000000  0.000000  2.314569e-23  0.027112   10    0\n",
       "MED-334   0.000000  0.000000  2.314569e-23  0.022925   10    0\n",
       "MED-335   0.000000  0.000000  2.314569e-23  0.020251   10    0\n",
       "MED-398   0.000000  0.000000  2.314569e-23  0.013594   10    0\n",
       "MED-557   0.000000  0.000000  2.314569e-23  0.013787   10    0\n",
       "MED-666   0.000000  0.000000  2.314569e-23  0.011882   10    0\n",
       "MED-691   0.000000  0.000000  2.314569e-23  0.017692   10    0\n",
       "MED-692   0.000000  0.000000  2.314569e-23  0.007089   10    0\n",
       "MED-702   0.000000  0.000000  2.314569e-23  0.019939   10    0\n",
       "MED-706   0.000000  0.000000  2.314569e-23  0.013718   10    0\n",
       "MED-707   0.000000  0.000000  2.314569e-23  0.037240   10    0\n",
       "MED-708   0.000000  0.000000  2.314569e-23  0.010120   10    0\n",
       "MED-709   0.000000  0.000000  2.314569e-23  0.020028   10    0\n",
       "MED-711   0.000000  0.000000  2.314569e-23  0.018396   10    0\n",
       "MED-712   0.000000  0.000000  2.314569e-23  0.013854   10    0\n",
       "MED-713   0.000000  0.000000  2.314569e-23  0.008278   10    0\n",
       "MED-714   0.000000  0.000000  2.314569e-23  0.032292   10    0\n",
       "MED-716   0.632461  0.975177  1.890716e-22  0.020736   10    0\n",
       "MED-717   0.000000  0.000000  2.314569e-23  0.015827   10    0\n",
       "MED-718   0.000000  0.000000  2.314569e-23  0.011552   10    0\n",
       "MED-719   0.000000  0.000000  2.314569e-23  0.013662   10    0\n",
       "MED-720   0.000000  0.000000  2.314569e-23  0.007473   10    0\n",
       "MED-721   0.617991  0.953745  1.616350e-22  0.022873   10    0\n",
       "...            ...       ...           ...       ...  ...  ...\n",
       "MED-5342  0.000000  0.000000  3.798837e-33  0.008694  999    0\n",
       "MED-5343  0.000000  0.000000  3.798837e-33  0.001546  999    0\n",
       "MED-5344  0.000000  0.000000  3.798837e-33  0.030737  999    0\n",
       "MED-5345  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5346  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5347  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5348  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5349  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5350  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5351  0.000000  0.000000  3.798837e-33  0.005216  999    0\n",
       "MED-5352  0.000000  0.000000  3.798837e-33  0.018953  999    0\n",
       "MED-5353  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5354  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5355  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5356  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5357  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5358  0.000000  0.000000  3.798837e-33  0.028846  999    0\n",
       "MED-5359  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5360  0.000000  0.000000  3.798837e-33  0.001277  999    0\n",
       "MED-5361  0.000000  0.000000  3.798837e-33  0.000816  999    0\n",
       "MED-5362  0.000000  0.000000  3.798837e-33  0.005114  999    0\n",
       "MED-5363  0.000000  0.000000  3.798837e-33  0.010336  999    0\n",
       "MED-5364  0.000000  0.000000  3.798837e-33  0.018986  999    0\n",
       "MED-5365  0.000000  0.000000  3.798837e-33  0.004488  999    0\n",
       "MED-5366  0.000000  0.000000  3.798837e-33  0.008911  999    0\n",
       "MED-5367  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5368  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5369  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5370  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "MED-5371  0.000000  0.000000  3.798837e-33  0.000000  999    0\n",
       "\n",
       "[9369528 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "      <th>bim25</th>\n",
       "      <th>unigram</th>\n",
       "      <th>cosine</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.027650e+06</td>\n",
       "      <td>1.027650e+06</td>\n",
       "      <td>1.027650e+06</td>\n",
       "      <td>1.027650e+06</td>\n",
       "      <td>1.027650e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.121619e-01</td>\n",
       "      <td>1.651769e-01</td>\n",
       "      <td>2.656210e-08</td>\n",
       "      <td>1.158282e-02</td>\n",
       "      <td>2.795699e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.264698e-01</td>\n",
       "      <td>5.390701e-01</td>\n",
       "      <td>8.205315e-06</td>\n",
       "      <td>1.065434e-02</td>\n",
       "      <td>2.311314e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.485328e-44</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.475276e-35</td>\n",
       "      <td>2.505088e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.436599e-31</td>\n",
       "      <td>9.550024e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.934189e-25</td>\n",
       "      <td>1.772193e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.071510e+01</td>\n",
       "      <td>1.984373e+01</td>\n",
       "      <td>6.253765e-03</td>\n",
       "      <td>1.456515e-01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tfidf         bim25       unigram        cosine           rel\n",
       "count  1.027650e+06  1.027650e+06  1.027650e+06  1.027650e+06  1.027650e+06\n",
       "mean   1.121619e-01  1.651769e-01  2.656210e-08  1.158282e-02  2.795699e-02\n",
       "std    3.264698e-01  5.390701e-01  8.205315e-06  1.065434e-02  2.311314e-01\n",
       "min    0.000000e+00  0.000000e+00  1.485328e-44  0.000000e+00  0.000000e+00\n",
       "25%    0.000000e+00  0.000000e+00  5.475276e-35  2.505088e-03  0.000000e+00\n",
       "50%    0.000000e+00  0.000000e+00  2.436599e-31  9.550024e-03  0.000000e+00\n",
       "75%    0.000000e+00  0.000000e+00  1.934189e-25  1.772193e-02  0.000000e+00\n",
       "max    1.071510e+01  1.984373e+01  6.253765e-03  1.456515e-01  3.000000e+00"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here you can see that rel is not always 0\n",
    "test_scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Full task: use the generated scores to train and evaluate point and pairwise models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores.to_pickle('queries/train_scores.pkl')\n",
    "dev_scores.to_pickle('queries/dev_scores.pkl')\n",
    "test_scores.to_pickle('queries/test_scores.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reduced task: create files according to rank lib documentation: https://sourceforge.net/p/lemur/wiki/RankLib%20File%20Format/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create csv for Ranklib, code taken from answer here: https://stackoverflow.com/questions/37439533/pandas-custom-file-format\n",
    "feature_columns = ['tfidf','bim25','unigram','cosine']\n",
    "cols2id = {col:str(i+1) for i,col in enumerate(feature_columns)}\n",
    "\n",
    "def f(x):\n",
    "    if x.name in feature_columns:\n",
    "        return cols2id[x.name] + ':' + x.astype(str)\n",
    "    elif x.name == 'qid':\n",
    "        return 'qid:' + x.astype(str)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "(train_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('reduced_task/train.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(dev_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('reduced_task/dev.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(test_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('reduced_task/test.csv', sep=' ', index=False, header=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3633"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many uniquely relevant documents do we have (or why does raw contain 5000 docs, and train/dev/test together only 3633)\n",
    "\n",
    "docids_train = train_rel.docid\n",
    "docids_dev = dev_rel.docid\n",
    "docids_test = test_rel.docid\n",
    "docids = pd.concat([docids_train, docids_dev, docids_test])\n",
    "len(docids.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
