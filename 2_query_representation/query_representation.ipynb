{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philipp\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Load the results from feature_generation_documents.ipynb, we will need them for computation\n",
    "#Note: pkl files are excluded from git for being to large, so you have to run other script once\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "tfidf = pd.read_pickle('../1_document_representation/pickle/tfidf.pkl')\n",
    "BIM25 = pd.read_pickle('../1_document_representation/pickle/BIM25.pkl')\n",
    "BIM25_alt = pd.read_pickle('../1_document_representation/pickle/BIM25_alt.pkl')\n",
    "unigram_LM = pd.read_pickle('../1_document_representation/pickle/unigramLM.pkl')\n",
    "idf = pd.read_pickle('../1_document_representation/pickle/idf.pkl')\n",
    "#We need this line to find the collection_vocabulary.py here, else we cannot load the col.pkl object\n",
    "import sys\n",
    "sys.path.append('../0_Collection_and_Inverted_Index/')\n",
    "from collection_vocabulary import Collection\n",
    "col=Collection()\n",
    "#Weighted Document Embeddings\n",
    "documents_fasttext = pd.read_pickle('../1_document_representation/pickle/documents_fasttext.pkl')\n",
    "documents_word2vec = pd.read_pickle('../1_document_representation/pickle/documents_word2vec.pkl')\n",
    "\n",
    "#For embeddings weighting of queries:\n",
    "fasttext_embeddings = pd.read_pickle('../1_document_representation/pickle/fasttext_embeddings.pkl')\n",
    "word2vec_embeddings = pd.read_pickle('../1_document_representation/pickle/word2vec_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample single term queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the same single term query  - \"cancer\". And compare the results of the three retrieval models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MED-1718    0.695754\n",
       "MED-4227    0.695754\n",
       "MED-4050    0.695754\n",
       "MED-5355    0.695754\n",
       "MED-4433    0.695754\n",
       "MED-5353    0.695754\n",
       "MED-5352    0.695754\n",
       "MED-3378    0.695754\n",
       "MED-3447    0.695754\n",
       "MED-1599    0.695754\n",
       "Name: cancer, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TFIDF\n",
    "a= tfidf.loc['cancer'].sort_values(ascending=False).head(10) # if you transpose you can directly select by the index term  > tf.transpose().cancer\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MED-3703    0.081339\n",
       "MED-2137    0.061650\n",
       "MED-2174    0.057772\n",
       "MED-4391    0.052210\n",
       "MED-890     0.048745\n",
       "MED-5184    0.048281\n",
       "MED-3551    0.047909\n",
       "MED-3555    0.047347\n",
       "MED-2258    0.045462\n",
       "MED-3699    0.044962\n",
       "Name: cancer, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unigram LM\n",
    "b= unigram_LM.loc['cancer'].sort_values(ascending=False).head(10)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MED-3703    0.923341\n",
       "MED-2174    0.891008\n",
       "MED-2137    0.887197\n",
       "MED-3555    0.884823\n",
       "MED-3699    0.883993\n",
       "MED-3551    0.881528\n",
       "MED-2258    0.877689\n",
       "MED-4391    0.877334\n",
       "MED-890     0.876812\n",
       "MED-5353    0.876768\n",
       "Name: cancer, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= BIM25.loc['cancer'].sort_values(ascending=False).head(10)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, there is very little overlap in the top 10 retrieved documents. Only the top-ranked doc of the probabilisitic ranking models matches.\n",
    "\n",
    "Now, let's get the query representations and compute the scores for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1              3\n",
      "count  110575.0  110575.000000\n",
      "mean        0.0       1.038698\n",
      "std         0.0       0.192874\n",
      "min         0.0       1.000000\n",
      "25%         0.0       1.000000\n",
      "50%         0.0       1.000000\n",
      "75%         0.0       1.000000\n",
      "max         0.0       2.000000\n",
      "             1             3\n",
      "count  12334.0  12334.000000\n",
      "mean       0.0      1.046700\n",
      "std        0.0      0.211004\n",
      "min        0.0      1.000000\n",
      "25%        0.0      1.000000\n",
      "50%        0.0      1.000000\n",
      "75%        0.0      1.000000\n",
      "max        0.0      2.000000\n"
     ]
    }
   ],
   "source": [
    "#Now to get the queries\n",
    "train_queries = pd.read_csv('../nfcorpus/train.all.queries', sep='\\t', header=None)\n",
    "train_queries.columns = ['id', 'text']\n",
    "dev_queries = pd.read_csv('../nfcorpus/dev.all.queries', sep='\\t', header=None)\n",
    "dev_queries.columns = ['id', 'text']\n",
    "test_queries = pd.read_csv('../nfcorpus/test.all.queries', sep='\\t', header=None)\n",
    "test_queries.columns = ['id', 'text']\n",
    "\n",
    "#And the relevance scores given\n",
    "train_rel = pd.read_csv('../nfcorpus/train.2-1-0.qrel', sep='\\t', header=None)\n",
    "print(train_rel.describe())\n",
    "test_rel = pd.read_csv('../nfcorpus/test.2-1-0.qrel', sep='\\t', header=None)\n",
    "print(test_rel.describe())\n",
    "dev_rel = pd.read_csv('../nfcorpus/dev.2-1-0.qrel', sep='\\t', header=None)\n",
    "#As we can see, column 1 is always 0, so drop it\n",
    "train_rel = train_rel.drop([1], axis=1)\n",
    "dev_rel = dev_rel.drop([1], axis=1)\n",
    "test_rel = test_rel.drop([1], axis=1)\n",
    "train_rel.columns = ['qid', 'docid', 'rel']\n",
    "dev_rel.columns = ['qid', 'docid', 'rel']\n",
    "test_rel.columns = ['qid', 'docid', 'rel']\n",
    "\n",
    "#The corpus also divides documents into train, dev and test, so we need to stick to that as well\n",
    "#(in order to get comparable results)\n",
    "train_docs = pd.read_csv('../nfcorpus/train.docs', sep='\\t', header=None)\n",
    "train_docs.columns = ['id', 'text']\n",
    "dev_docs = pd.read_csv('../nfcorpus/dev.docs', sep='\\t', header=None)\n",
    "dev_docs.columns = ['id', 'text']\n",
    "test_docs = pd.read_csv('../nfcorpus/test.docs', sep='\\t', header=None)\n",
    "test_docs.columns = ['id', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tfidf with cosine similarity, we need the tfidf vectors for each query (using idf of our document corpus and tf for each query):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can skip this if you already did it once, just start loading the matrices from pkl files\n",
    "def get_query_term_matrix(queries, col):\n",
    "    query_term_matrix = []\n",
    "    for query in queries.itertuples():\n",
    "        tf_vector =[]\n",
    "        for word in col.vocabulary:\n",
    "            n= query.text.count(word)\n",
    "            tf_vector.append(n)\n",
    "        query_term_matrix.append(tf_vector)\n",
    "    return pd.DataFrame(data=query_term_matrix,index=queries.id,columns=col.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's compute the term_matrix for our query texts\n",
    "train_matrix = get_query_term_matrix(train_queries, col)\n",
    "dev_matrix = get_query_term_matrix(dev_queries, col)\n",
    "test_matrix = get_query_term_matrix(test_queries, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'hort</th>\n",
       "      <th>+</th>\n",
       "      <th>-</th>\n",
       "      <th>--a</th>\n",
       "      <th>--all</th>\n",
       "      <th>--have</th>\n",
       "      <th>--mainly</th>\n",
       "      <th>--of</th>\n",
       "      <th>--showed</th>\n",
       "      <th>--the</th>\n",
       "      <th>...</th>\n",
       "      <th>zooplankton</th>\n",
       "      <th>zoxazolamine</th>\n",
       "      <th>zr</th>\n",
       "      <th>zu</th>\n",
       "      <th>zuccarini</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zugesetztem</th>\n",
       "      <th>zusatzstoffe-online</th>\n",
       "      <th>zygote</th>\n",
       "      <th>zymography</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PLAIN-1008</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAIN-1018</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAIN-102</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAIN-1028</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLAIN-1039</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29052 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            'hort  +   -  --a  --all  --have  --mainly  --of  --showed  --the  \\\n",
       "id                                                                              \n",
       "PLAIN-1008      0  0   4    0      0       0         0     0         0      0   \n",
       "PLAIN-1018      0  0  11    0      0       0         0     0         0      0   \n",
       "PLAIN-102       0  1  54    0      0       0         0     0         0      0   \n",
       "PLAIN-1028      0  0   5    0      0       0         0     0         0      0   \n",
       "PLAIN-1039      0  0   4    0      0       0         0     0         0      0   \n",
       "\n",
       "               ...      zooplankton  zoxazolamine  zr  zu  zuccarini  \\\n",
       "id             ...                                                     \n",
       "PLAIN-1008     ...                0             0   0   0          0   \n",
       "PLAIN-1018     ...                0             0   0   0          0   \n",
       "PLAIN-102      ...                0             0   0   0          0   \n",
       "PLAIN-1028     ...                0             0   0   0          0   \n",
       "PLAIN-1039     ...                0             0   0   0          0   \n",
       "\n",
       "            zucchini  zugesetztem  zusatzstoffe-online  zygote  zymography  \n",
       "id                                                                          \n",
       "PLAIN-1008         0            0                    0       0           0  \n",
       "PLAIN-1018         0            0                    0       0           0  \n",
       "PLAIN-102          0            0                    0       0           0  \n",
       "PLAIN-1028         0            0                    0       0           0  \n",
       "PLAIN-1039         0            0                    0       0           0  \n",
       "\n",
       "[5 rows x 29052 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another speed up, save the matrices\n",
    "train_matrix.to_pickle('pickle/train_matrix.pkl')\n",
    "dev_matrix.to_pickle('pickle/dev_matrix.pkl')\n",
    "test_matrix.to_pickle('pickle/test_matrix.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you already ran this once, you can simply load the matrices instead of computing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = pd.read_pickle('pickle/train_matrix.pkl')\n",
    "dev_matrix = pd.read_pickle('pickle/dev_matrix.pkl')\n",
    "test_matrix = pd.read_pickle('pickle/test_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As seen in the tf idf computation of the documents, we will use an inverted matrix\n",
    "train_matrix_inverted = train_matrix.transpose()\n",
    "dev_matrix_inverted = dev_matrix.transpose()\n",
    "test_matrix_inverted = test_matrix.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_query_tfidf(inv_query_matrix, idf):\n",
    "    #TF\n",
    "    # nominator part\n",
    "    nominator=inv_query_matrix.mask(inv_query_matrix!=0,other=(np.log10(inv_query_matrix)+1))\n",
    "    # denominator part\n",
    "    most_frequent_term=inv_query_matrix.max(axis=0) # determine most frequent term in each query\n",
    "    denominator= np.log10(most_frequent_term)\n",
    "    denominator+=1\n",
    "    tf=nominator.div(denominator, axis=1)\n",
    "    tfidf_query= tf.mul(idf, axis=0) # we multiply the tf scores in every query with the corresponding idf scores\n",
    "    return tfidf_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philipp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log10\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Now, let's get the tfidf scores for each query, please ignore the error\n",
    "train_tfidf = compute_query_tfidf(train_matrix_inverted, idf)\n",
    "dev_tfidf = compute_query_tfidf(dev_matrix_inverted, idf)\n",
    "test_tfidf = compute_query_tfidf(test_matrix_inverted, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save those as pkl as well\n",
    "train_tfidf.to_pickle('pickle/train_tfidf.pkl')\n",
    "dev_tfidf.to_pickle('pickle/dev_tfidf.pkl')\n",
    "test_tfidf.to_pickle('pickle/test_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again, load it to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or load them, if they already exist\n",
    "train_tfidf = pd.read_pickle('pickle/train_tfidf.pkl')\n",
    "dev_tfidf = pd.read_pickle('pickle/dev_tfidf.pkl')\n",
    "test_tfidf = pd.read_pickle('pickle/test_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>PLAIN-1008</th>\n",
       "      <th>PLAIN-1018</th>\n",
       "      <th>PLAIN-102</th>\n",
       "      <th>PLAIN-1028</th>\n",
       "      <th>PLAIN-1039</th>\n",
       "      <th>PLAIN-1050</th>\n",
       "      <th>PLAIN-1066</th>\n",
       "      <th>PLAIN-1078</th>\n",
       "      <th>PLAIN-1088</th>\n",
       "      <th>PLAIN-1098</th>\n",
       "      <th>...</th>\n",
       "      <th>PLAIN-91</th>\n",
       "      <th>PLAIN-913</th>\n",
       "      <th>PLAIN-924</th>\n",
       "      <th>PLAIN-934</th>\n",
       "      <th>PLAIN-946</th>\n",
       "      <th>PLAIN-956</th>\n",
       "      <th>PLAIN-966</th>\n",
       "      <th>PLAIN-977</th>\n",
       "      <th>PLAIN-987</th>\n",
       "      <th>PLAIN-997</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'hort</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>2.529077</td>\n",
       "      <td>2.433363</td>\n",
       "      <td>2.366582</td>\n",
       "      <td>2.628729</td>\n",
       "      <td>2.621103</td>\n",
       "      <td>2.5047</td>\n",
       "      <td>2.604743</td>\n",
       "      <td>2.726138</td>\n",
       "      <td>2.44186</td>\n",
       "      <td>2.918655</td>\n",
       "      <td>...</td>\n",
       "      <td>1.971896</td>\n",
       "      <td>1.79211</td>\n",
       "      <td>2.682064</td>\n",
       "      <td>1.856777</td>\n",
       "      <td>2.744308</td>\n",
       "      <td>2.022176</td>\n",
       "      <td>2.561169</td>\n",
       "      <td>2.587771</td>\n",
       "      <td>2.434983</td>\n",
       "      <td>2.680533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--a</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--all</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id     PLAIN-1008  PLAIN-1018  PLAIN-102  PLAIN-1028  PLAIN-1039  PLAIN-1050  \\\n",
       "'hort    0.000000    0.000000   0.000000    0.000000    0.000000      0.0000   \n",
       "+        0.000000    0.000000   0.750049    0.000000    0.000000      0.0000   \n",
       "-        2.529077    2.433363   2.366582    2.628729    2.621103      2.5047   \n",
       "--a      0.000000    0.000000   0.000000    0.000000    0.000000      0.0000   \n",
       "--all    0.000000    0.000000   0.000000    0.000000    0.000000      0.0000   \n",
       "\n",
       "id     PLAIN-1066  PLAIN-1078  PLAIN-1088  PLAIN-1098    ...      PLAIN-91  \\\n",
       "'hort    0.000000    0.000000     0.00000    0.000000    ...      0.000000   \n",
       "+        0.000000    0.000000     0.00000    0.000000    ...      0.000000   \n",
       "-        2.604743    2.726138     2.44186    2.918655    ...      1.971896   \n",
       "--a      0.000000    0.000000     0.00000    0.000000    ...      0.000000   \n",
       "--all    0.000000    0.000000     0.00000    0.000000    ...      0.000000   \n",
       "\n",
       "id     PLAIN-913  PLAIN-924  PLAIN-934  PLAIN-946  PLAIN-956  PLAIN-966  \\\n",
       "'hort    0.00000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "+        0.00000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "-        1.79211   2.682064   1.856777   2.744308   2.022176   2.561169   \n",
       "--a      0.00000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "--all    0.00000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "id     PLAIN-977  PLAIN-987  PLAIN-997  \n",
       "'hort   0.000000   0.000000   0.000000  \n",
       "+       0.000000   0.000000   0.000000  \n",
       "-       2.587771   2.434983   2.680533  \n",
       "--a     0.000000   0.000000   0.000000  \n",
       "--all   0.000000   0.000000   0.000000  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As for the embeddings, we also need to weight them with their tfidf for each query, in order to compute similarity between document and queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_embeddings(embeddings, tfidf_embed):\n",
    "    sum_of_tfidf_weights=tfidf_embed.sum(axis=0)#vector containing the normalizing constant for each doc\n",
    "    embeddings_dict={}\n",
    "    # we have to make use of the following workaround to avoid memory errors\n",
    "    # 1. calculate 100d embeddings vector for each doc/query and store it in dictionary\n",
    "    # 2. recreate a a dataframe containg the embeddings for all docs/queries from the dictionary\n",
    "    for doc in tfidf_embed.columns:\n",
    "        if doc not in embeddings_dict.keys():\n",
    "            embedding=(tfidf_embed[doc].mask(tfidf_embed[doc]!=0, other=(tfidf_embed[doc]*embeddings)).sum(axis=0))/sum_of_tfidf_weights[doc]\n",
    "            embeddings_dict[doc]=embedding\n",
    "        else:\n",
    "            print('calculated embeddings successfully and stored them in dictionary')\n",
    "    weighted_embedding = pd.DataFrame.from_dict(embeddings_dict)\n",
    "    return weighted_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fasttext Embeddings\n",
    "train_queries_fasttext = get_weighted_embeddings(fasttext_embeddings, train_tfidf)\n",
    "dev_queries_fasttext = get_weighted_embeddings(fasttext_embeddings, dev_tfidf)\n",
    "test_queries_fasttext = get_weighted_embeddings(fasttext_embeddings, test_tfidf)\n",
    "\n",
    "#Save them for later speed up\n",
    "train_queries_fasttext.to_pickle('pickle/train_queries_fasttext.pkl')\n",
    "dev_queries_fasttext.to_pickle('pickle/dev_queries_fasttext.pkl')\n",
    "test_queries_fasttext.to_pickle('pickle/test_queries_fasttext.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec Embeddings\n",
    "train_queries_word2vec = get_weighted_embeddings(word2vec_embeddings, train_tfidf)\n",
    "dev_queries_word2vec = get_weighted_embeddings(word2vec_embeddings, dev_tfidf)\n",
    "test_queries_word2vec = get_weighted_embeddings(word2vec_embeddings, test_tfidf)\n",
    "\n",
    "#Save them as well\n",
    "train_queries_word2vec.to_pickle('pickle/train_queries_word2vec.pkl')\n",
    "dev_queries_word2vec.to_pickle('pickle/dev_queries_word2vec.pkl')\n",
    "test_queries_word2vec.to_pickle('pickle/test_queries_word2vec.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed up: Load the weighted embeddings, instead of computing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fasttext Embeddings\n",
    "train_queries_fasttext = pd.read_pickle('pickle/train_queries_fasttext.pkl')\n",
    "dev_queries_fasttext = pd.read_pickle('pickle/dev_queries_fasttext.pkl')\n",
    "test_queries_fasttext = pd.read_pickle('pickle/test_queries_fasttext.pkl')\n",
    "\n",
    "#Word2Vec Embeddings\n",
    "train_queries_word2vec = pd.read_pickle('pickle/train_queries_word2vec.pkl')\n",
    "dev_queries_word2vec = pd.read_pickle('pickle/dev_queries_word2vec.pkl')\n",
    "test_queries_word2vec = pd.read_pickle('pickle/test_queries_word2vec.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, put it all together to compute the scores for every query document pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(queries, documents, rel, queries_tfidf, queries_fasttext, queries_word2vec):\n",
    "    #Get the documents defined in the nfcorpous\n",
    "    doc_keys = documents.id\n",
    "    tfidf_part = tfidf.loc[:, doc_keys]\n",
    "    BIM25_part = BIM25.loc[:, doc_keys]\n",
    "    BIM25_alt_part = BIM25_alt.loc[:, doc_keys]\n",
    "    unigram_LM_part = unigram_LM.loc[:, doc_keys]\n",
    "    documents_fasttext_part = documents_fasttext.loc[:, doc_keys]\n",
    "    documents_word2vec_part = documents_word2vec.loc[:, doc_keys]\n",
    "    list_of_df = []\n",
    "    query_keys = queries['id']\n",
    "    print('Computing', len(query_keys), 'queries on', len(doc_keys), 'documents')\n",
    "    \n",
    "    #Get the cosine between queries and docs (much faster than inside the loop)\n",
    "    cosine = cosine_similarity(queries_tfidf.transpose(), tfidf_part.transpose())\n",
    "    #Also, cosine between the embeddings:\n",
    "    fasttext = cosine_similarity(queries_fasttext.transpose(), documents_fasttext_part.transpose())\n",
    "    word2vec = cosine_similarity(queries_word2vec.transpose(), documents_word2vec_part.transpose())\n",
    "    print('Cosines computed, start iterating...')\n",
    "    i = 0\n",
    "    for key in query_keys:\n",
    "        text = str(queries.loc[queries['id'] == key].text)\n",
    "        tfidf_scores = tfidf_part.loc[text.split()].sum()\n",
    "        bim25_scores = BIM25_part.loc[text.split()].sum()\n",
    "        bim25_alt_scores = BIM25_alt_part.loc[text.split()].sum()\n",
    "        unigram_scores = unigram_LM_part.loc[text.split()].product()\n",
    "        total = pd.DataFrame()\n",
    "        total['tfidf'] = tfidf_scores\n",
    "        total['bim25'] = bim25_scores\n",
    "        total['bim25_alt'] = bim25_alt_scores\n",
    "        total['unigram'] = unigram_scores\n",
    "        total['cosine'] = cosine[i]\n",
    "        total['fasttext'] = fasttext[i]\n",
    "        total['word2vec'] = word2vec[i]\n",
    "        total['qid'] = key.replace('PLAIN-', '')\n",
    "        #Rel only contains 1 and 2, everything that is not in there is set to 0\n",
    "        total['rel'] = 0\n",
    "        rel_temp = rel.loc[(rel['qid'] == key)]\n",
    "        for row in rel_temp.itertuples():\n",
    "            total.at[row.docid, 'rel'] = row.rel\n",
    "        total.set_index(np.arange(len(doc_keys)))\n",
    "        total.rename(columns={'': 'docid'}, inplace=True)\n",
    "        list_of_df.append(total)\n",
    "        i+=1\n",
    "        if (i%100 == 0):\n",
    "            print(i, 'queries computed')\n",
    "    scores = pd.concat(list_of_df)\n",
    "    print(i, 'queries computed')\n",
    "    #Sanity check: should be same\n",
    "    print(len(scores))\n",
    "    print(len(doc_keys)*len(query_keys))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "def cosine_similarity(query, docs):\n",
    "    cos_similarity = 1-distance.cdist(query, docs, metric='cosine')\n",
    "    return cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 2594 queries on 3612 documents\n",
      "Cosines computed, start iterating...\n",
      "100 queries computed\n",
      "200 queries computed\n",
      "300 queries computed\n",
      "400 queries computed\n",
      "500 queries computed\n",
      "600 queries computed\n",
      "700 queries computed\n",
      "800 queries computed\n",
      "900 queries computed\n",
      "1000 queries computed\n",
      "1100 queries computed\n",
      "1200 queries computed\n",
      "1300 queries computed\n",
      "1400 queries computed\n",
      "1500 queries computed\n",
      "1600 queries computed\n",
      "1700 queries computed\n",
      "1800 queries computed\n",
      "1900 queries computed\n",
      "2000 queries computed\n",
      "2100 queries computed\n",
      "2200 queries computed\n",
      "2300 queries computed\n",
      "2400 queries computed\n",
      "2500 queries computed\n",
      "2594 queries computed\n",
      "9369528\n",
      "9369528\n",
      "Computing 325 queries on 3193 documents\n",
      "Cosines computed, start iterating...\n",
      "100 queries computed\n",
      "200 queries computed\n",
      "300 queries computed\n",
      "325 queries computed\n",
      "1037725\n",
      "1037725\n",
      "Computing 325 queries on 3162 documents\n",
      "Cosines computed, start iterating...\n",
      "100 queries computed\n",
      "200 queries computed\n",
      "300 queries computed\n",
      "325 queries computed\n",
      "1027650\n",
      "1027650\n"
     ]
    }
   ],
   "source": [
    "train_scores = compute_scores(train_queries, train_docs, train_rel, train_tfidf, train_queries_fasttext, train_queries_word2vec)\n",
    "dev_scores = compute_scores(dev_queries, dev_docs, dev_rel, dev_tfidf, dev_queries_fasttext, dev_queries_word2vec)\n",
    "test_scores = compute_scores(test_queries, test_docs, test_rel, test_tfidf, test_queries_fasttext, test_queries_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "      <th>bim25</th>\n",
       "      <th>bim25_alt</th>\n",
       "      <th>unigram</th>\n",
       "      <th>cosine</th>\n",
       "      <th>fasttext</th>\n",
       "      <th>word2vec</th>\n",
       "      <th>qid</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MED-10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.015284</td>\n",
       "      <td>0.888365</td>\n",
       "      <td>0.878828</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>0.870183</td>\n",
       "      <td>0.860293</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-118</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.024806</td>\n",
       "      <td>0.934861</td>\n",
       "      <td>0.932443</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-301</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.923050</td>\n",
       "      <td>0.906691</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MED-306</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.314569e-23</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.955033</td>\n",
       "      <td>0.946509</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tfidf  bim25  bim25_alt       unigram    cosine  fasttext  word2vec  \\\n",
       "MED-10     0.0    0.0        0.0  2.314569e-23  0.015284  0.888365  0.878828   \n",
       "MED-14     0.0    0.0        0.0  2.314569e-23  0.010135  0.870183  0.860293   \n",
       "MED-118    0.0    0.0        0.0  2.314569e-23  0.024806  0.934861  0.932443   \n",
       "MED-301    0.0    0.0        0.0  2.314569e-23  0.019779  0.923050  0.906691   \n",
       "MED-306    0.0    0.0        0.0  2.314569e-23  0.030273  0.955033  0.946509   \n",
       "\n",
       "        qid  rel  \n",
       "MED-10   10    0  \n",
       "MED-14   10    0  \n",
       "MED-118  10    0  \n",
       "MED-301  10    0  \n",
       "MED-306  10    0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "      <th>bim25</th>\n",
       "      <th>bim25_alt</th>\n",
       "      <th>unigram</th>\n",
       "      <th>cosine</th>\n",
       "      <th>fasttext</th>\n",
       "      <th>word2vec</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.027650e+06</td>\n",
       "      <td>1.027650e+06</td>\n",
       "      <td>1.027650e+06</td>\n",
       "      <td>1.027650e+06</td>\n",
       "      <td>1.027650e+06</td>\n",
       "      <td>1.027650e+06</td>\n",
       "      <td>1.027650e+06</td>\n",
       "      <td>1.027650e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.121619e-01</td>\n",
       "      <td>1.650738e-01</td>\n",
       "      <td>1.594483e-01</td>\n",
       "      <td>2.656210e-08</td>\n",
       "      <td>1.158282e-02</td>\n",
       "      <td>9.107293e-01</td>\n",
       "      <td>8.926954e-01</td>\n",
       "      <td>1.256264e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.264698e-01</td>\n",
       "      <td>5.390886e-01</td>\n",
       "      <td>5.161146e-01</td>\n",
       "      <td>8.205315e-06</td>\n",
       "      <td>1.065434e-02</td>\n",
       "      <td>4.365257e-02</td>\n",
       "      <td>4.860128e-02</td>\n",
       "      <td>1.163006e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.485328e-44</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.786127e-01</td>\n",
       "      <td>5.770608e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.475276e-35</td>\n",
       "      <td>2.505088e-03</td>\n",
       "      <td>8.860722e-01</td>\n",
       "      <td>8.633588e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.436599e-31</td>\n",
       "      <td>9.550024e-03</td>\n",
       "      <td>9.182195e-01</td>\n",
       "      <td>9.002982e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.934189e-25</td>\n",
       "      <td>1.772193e-02</td>\n",
       "      <td>9.433185e-01</td>\n",
       "      <td>9.297699e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.071510e+01</td>\n",
       "      <td>1.983245e+01</td>\n",
       "      <td>1.821768e+01</td>\n",
       "      <td>6.253765e-03</td>\n",
       "      <td>1.456515e-01</td>\n",
       "      <td>9.930003e-01</td>\n",
       "      <td>9.915299e-01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tfidf         bim25     bim25_alt       unigram        cosine  \\\n",
       "count  1.027650e+06  1.027650e+06  1.027650e+06  1.027650e+06  1.027650e+06   \n",
       "mean   1.121619e-01  1.650738e-01  1.594483e-01  2.656210e-08  1.158282e-02   \n",
       "std    3.264698e-01  5.390886e-01  5.161146e-01  8.205315e-06  1.065434e-02   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  1.485328e-44  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  5.475276e-35  2.505088e-03   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  2.436599e-31  9.550024e-03   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  1.934189e-25  1.772193e-02   \n",
       "max    1.071510e+01  1.983245e+01  1.821768e+01  6.253765e-03  1.456515e-01   \n",
       "\n",
       "           fasttext      word2vec           rel  \n",
       "count  1.027650e+06  1.027650e+06  1.027650e+06  \n",
       "mean   9.107293e-01  8.926954e-01  1.256264e-02  \n",
       "std    4.365257e-02  4.860128e-02  1.163006e-01  \n",
       "min    5.786127e-01  5.770608e-01  0.000000e+00  \n",
       "25%    8.860722e-01  8.633588e-01  0.000000e+00  \n",
       "50%    9.182195e-01  9.002982e-01  0.000000e+00  \n",
       "75%    9.433185e-01  9.297699e-01  0.000000e+00  \n",
       "max    9.930003e-01  9.915299e-01  2.000000e+00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reduced task: create files according to rank lib documentation: https://sourceforge.net/p/lemur/wiki/RankLib%20File%20Format/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create csv for Ranklib, code taken from answer here: https://stackoverflow.com/questions/37439533/pandas-custom-file-format\n",
    "feature_columns = ['tfidf','bim25','bim25_alt','unigram','cosine','fasttext','word2vec']\n",
    "cols2id = {col:str(i+1) for i,col in enumerate(feature_columns)}\n",
    "\n",
    "def f(x):\n",
    "    if x.name in feature_columns:\n",
    "        return cols2id[x.name] + ':' + x.astype(str)\n",
    "    elif x.name == 'qid':\n",
    "        return 'qid:' + x.astype(str)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "(train_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/train.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(dev_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/dev.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(test_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/test.csv', sep=' ', index=False, header=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get a baseline for each of our features, we also build models just using one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['tfidf']\n",
    "cols2id = {col:str(i+1) for i,col in enumerate(feature_columns)}\n",
    "\n",
    "def f(x):\n",
    "    if x.name in feature_columns:\n",
    "        return cols2id[x.name] + ':' + x.astype(str)\n",
    "    elif x.name == 'qid':\n",
    "        return 'qid:' + x.astype(str)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "(train_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/train_tfidf.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(dev_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/dev_tfidf.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(test_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/test_tfidf.csv', sep=' ', index=False, header=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['bim25']\n",
    "cols2id = {col:str(i+1) for i,col in enumerate(feature_columns)}\n",
    "\n",
    "def f(x):\n",
    "    if x.name in feature_columns:\n",
    "        return cols2id[x.name] + ':' + x.astype(str)\n",
    "    elif x.name == 'qid':\n",
    "        return 'qid:' + x.astype(str)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "(train_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/train_bm25.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(dev_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/dev_bm25.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(test_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/test_bm25.csv', sep=' ', index=False, header=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['bim25_alt']\n",
    "cols2id = {col:str(i+1) for i,col in enumerate(feature_columns)}\n",
    "\n",
    "def f(x):\n",
    "    if x.name in feature_columns:\n",
    "        return cols2id[x.name] + ':' + x.astype(str)\n",
    "    elif x.name == 'qid':\n",
    "        return 'qid:' + x.astype(str)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "(train_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/train_bm25_alt.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(dev_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/dev_bm25_alt.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(test_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/test_bm25_alt.csv', sep=' ', index=False, header=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['unigram']\n",
    "cols2id = {col:str(i+1) for i,col in enumerate(feature_columns)}\n",
    "\n",
    "def f(x):\n",
    "    if x.name in feature_columns:\n",
    "        return cols2id[x.name] + ':' + x.astype(str)\n",
    "    elif x.name == 'qid':\n",
    "        return 'qid:' + x.astype(str)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "(train_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/train_unigram.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(dev_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/dev_unigram.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(test_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/test_unigram.csv', sep=' ', index=False, header=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['cosine']\n",
    "cols2id = {col:str(i+1) for i,col in enumerate(feature_columns)}\n",
    "\n",
    "def f(x):\n",
    "    if x.name in feature_columns:\n",
    "        return cols2id[x.name] + ':' + x.astype(str)\n",
    "    elif x.name == 'qid':\n",
    "        return 'qid:' + x.astype(str)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "(train_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/train_cosine.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(dev_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/dev_cosine.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(test_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/test_cosine.csv', sep=' ', index=False, header=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['fasttext']\n",
    "cols2id = {col:str(i+1) for i,col in enumerate(feature_columns)}\n",
    "\n",
    "def f(x):\n",
    "    if x.name in feature_columns:\n",
    "        return cols2id[x.name] + ':' + x.astype(str)\n",
    "    elif x.name == 'qid':\n",
    "        return 'qid:' + x.astype(str)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "(train_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/train_fasttext.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(dev_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/dev_fasttext.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(test_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/test_fasttext.csv', sep=' ', index=False, header=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['word2vec']\n",
    "cols2id = {col:str(i+1) for i,col in enumerate(feature_columns)}\n",
    "\n",
    "def f(x):\n",
    "    if x.name in feature_columns:\n",
    "        return cols2id[x.name] + ':' + x.astype(str)\n",
    "    elif x.name == 'qid':\n",
    "        return 'qid:' + x.astype(str)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "(train_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/train_word2vec.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(dev_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/dev_word2vec.csv', sep=' ', index=False, header=None)\n",
    ")\n",
    "(test_scores.apply(lambda x: f(x))[['rel','qid'] + feature_columns]\n",
    "  .to_csv('../3_ranklib_framework/baseline/test_word2vec.csv', sep=' ', index=False, header=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
